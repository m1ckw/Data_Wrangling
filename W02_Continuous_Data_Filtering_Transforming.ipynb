{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d6cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc24910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36787944, 1.        , 2.71828183, 7.3890561 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp([-1, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f7cd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.exp([-1, 0, 1, 2]))  # the natural logarithm is the inverse of exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509e9fd",
   "metadata": {},
   "source": [
    "## Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d736a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([157. , 167.4, 159.6, 168.5, 147.8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights = np.loadtxt(\"https://raw.githubusercontent.com/gagolews/\" +\n",
    "    \"teaching-data/master/marek/nhanes_adult_female_height_2020.txt\")\n",
    "heights[-5:] # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79677029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160.13679222932953, 7.062021850008261)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(heights), np.std(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5585e99",
   "metadata": {},
   "source": [
    "A standardised version of a vector $(x_1,\\dots,x_n)$ consists of subtracting from each element the sample arithmetic mean (which we call centring) and then dividing it by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c402a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44417764,  1.02848843, -0.07601113,  1.18425119, -1.74692071])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A standardised version of a vector $(x_1,\\dots,x_n)$... \n",
    "\n",
    "heights_std = (heights-np.mean(heights))/np.std(heights)\n",
    "heights_std[-5:] # preview\n",
    "\n",
    "# This gives us our Z Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab53229",
   "metadata": {},
   "source": [
    "* z-score of 0 corresponds to an observation equal to the sample mean (perfectly average);\n",
    "* z-score of 1 is obtained for a datum 1 standard deviation above the mean;\n",
    "* z-score of -2 means that it is a value 2 standard deviations below the mean;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffdc4efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8920872660373198e-15, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(heights_std), np.std(heights_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e210b",
   "metadata": {},
   "source": [
    "## Min-Max Scaling and Clipping\n",
    "\n",
    "A less frequent, but still noteworthy, transformation is called min-max scaling and involves subtracting the minimum and then dividing by the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85865913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.4  , 1.   , 0.034, 0.35 , 0.46 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1.5, 0.5, 3.5, -1.33, 0.25, 0.8])\n",
    "(x - np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e619d",
   "metadata": {},
   "source": [
    "Here, the smallest value is mapped to 0, and the largest one is equal to 1.\n",
    "\n",
    "Note that 0.5 does not mean that the value is equal to the mean (unless we are very lucky!).\n",
    "\n",
    "Also, clipping can be used to replace all values less than 0 with 0 and those greater than 1 with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12a078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.5 , 1.  , 0.  , 0.25, 0.8 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1add70",
   "metadata": {},
   "source": [
    "The function is, of course, flexible; another popular choice is clipping to [-1, 1].\n",
    "\n",
    "This can be implemented manually by means of the vectorised pairwise minimum and maximum functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342987ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.5 , 1.  , 0.  , 0.25, 0.8 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.minimum(1, np.maximum(0, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abceb60",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "Normalisation is the scaling of a given vector so that it is of unit length. Usually, by length we mean the square root of the sum of squares, i.e., the Euclidean norm whose special case for n=2 we know well from high school: the length of a vector $(a,b)$ is $\\sqrt{a^2+b^2}$, e.g., $\\|(1, 2)\\| = \\sqrt{5} \\simeq 2.236$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1052a25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13834289,  0.69171446, -0.55337157,  0.27668579,  0.34585723])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 5, -4, 2, 2.5])\n",
    "x/np.sqrt(np.sum(x**2))  # x divided by the Euclidean norm of x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633336e",
   "metadata": {},
   "source": [
    "Note that normalisation is pretty similar to standardisation if data are already centred (when the mean was subtracted). Actually, we can obtain one from the other by scaling by $\\sqrt{n-1}$.\n",
    "\n",
    "At other times, by length we can also mean the Manhattan norm,\n",
    "being the sum of absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ab5149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06896552,  0.34482759, -0.27586207,  0.13793103,  0.17241379])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / np.sum(np.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d2d28",
   "metadata": {},
   "source": [
    "This is frequently applied upon vectors of nonnegative values, whose normalised versions can be interpreted as probabilities: values between 0 and 1 which additionally add up to 1 (or, equivalently, 100%). In particular, on binned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b29a21bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 306 1776 1773  366]\n"
     ]
    }
   ],
   "source": [
    "c, b = np.histogram(heights, [-np.inf, 150, 160, 170, np.inf])\n",
    "print(c)  # counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17936964",
   "metadata": {},
   "source": [
    "And now, converting the counts to empirical probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caab8add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07249467 0.42075338 0.42004264 0.08670931]\n"
     ]
    }
   ],
   "source": [
    "p = c/np.sum(c)\n",
    "print(p)\n",
    "\n",
    "# We did not apply numpy.abs, because the values were already nonnegative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933e0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
